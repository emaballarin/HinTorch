{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Deep Learning *slipping* [on the shoulders of Giants](https://www.wikiwand.com/en/Standing_on_the_shoulders_of_giants)?\n",
    "### A *critical* reproducibility challenge for [[Rumelhart et al., 1986]](https://sci-hub.se/10.1038/323533a0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seminal paper *Learning representations by back-propagating errors* (Rumelhart *et al.*, 1986) is considered a staple result in modern *deep learning*. However, at the time, *reproducible science* was not a popular paradigm as it is ([or... is it?](https://www.technologyreview.com/2020/11/12/1011944/artificial-intelligence-replication-crisis-science-big-tech-google-deepmind-facebook-openai/)) today: indeed, some information is missing from the paper preventing its full reproducibility (*e.g.* an exact specification of the activation function used in the *mirror symmetry experiment*), and the original code has never been shared - as it is instead [recommended by some venues today](https://www.nature.com/articles/s42256-019-0092-6) upon publication.  \n",
    "\n",
    "Anyway, one would expect that such straightforward (and crucial!) element as the *mirror symmetry experiment* is indeed easily and successfully verified. What a fun little challenge! üòÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily compute statistics of arrays\n",
    "import numpy as np\n",
    "\n",
    "# Type hints\n",
    "from typing import Iterable, List, Tuple\n",
    "from torch import Tensor\n",
    "\n",
    "# Tensors and NNs\n",
    "import torch as th\n",
    "from ebtorch.nn import FCBlock  # API for fully-connected NN blocks\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimisers\n",
    "import torch.optim as optim\n",
    "\n",
    "# Tensor data[sets|loader]s\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Iterable handling\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "\n",
    "# Utilities for callables\n",
    "from ebtorch.nn.utils import argser_f, emplace_kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithmic function (symmetry-detection for 6-sized binary inputs) we\n",
    "# want to approximate with a NN\n",
    "def is_symmetric(iterable: Iterable) -> float:\n",
    "    assert len(iterable) == 6\n",
    "    if (\n",
    "        # iterable[0:3] == iterable[5:2:-1] still unsupported for PyTorch tensors, sadly!\n",
    "        iterable[0] == iterable[-1]\n",
    "        and iterable[1] == iterable[-2]\n",
    "        and iterable[2] == iterable[-3]\n",
    "    ):\n",
    "    # 1 == Yes | 0 == No\n",
    "        return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the dataset output-wise early on, to be able to balance it later in\n",
    "# case we need to.\n",
    "\n",
    "x_all: List[Tuple[float]] = [item for item in product([0.0, 1.0], repeat=6)]\n",
    "x_symmetric: List[Tuple[float]] = [item for item in x_all if is_symmetric(item)]\n",
    "x_non_symmetric: List[Tuple[float]] = [item for item in set(x_all).difference(set(x_symmetric))]\n",
    "\n",
    "# And we tensorise it\n",
    "del x_all\n",
    "x_symmetric: Tensor = th.tensor(x_symmetric, dtype=th.float32)\n",
    "x_non_symmetric: Tensor = th.tensor(x_non_symmetric, dtype=th.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unbalanced dataset tensor\n",
    "x: Tensor = th.cat((x_non_symmetric, x_symmetric), dim=0)\n",
    "y: Tensor = th.tensor([[is_symmetric(sub_x)] for sub_x in x])\n",
    "\n",
    "# And the balanced one\n",
    "balancing_ratio: int = int((x_non_symmetric.shape[0]/x_symmetric.shape[0]))\n",
    "x_balanced: Tensor = th.cat((x_non_symmetric, th.cat([x_symmetric]*balancing_ratio, dim=0)), dim=0)\n",
    "y_balanced: Tensor = th.tensor([[is_symmetric(sub_x)] for sub_x in x_balanced])\n",
    "del balancing_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to proper PyTorch data[set|loader]s\n",
    "\n",
    "# Datasets\n",
    "train_unbalanced_ds: TensorDataset = TensorDataset(x, y)\n",
    "train_balanced_ds: TensorDataset = TensorDataset(x_balanced, y_balanced)\n",
    "\n",
    "# Dataloaders (we do full-dataset-batching as in the paper)\n",
    "train_unbalanced_dl: DataLoader = DataLoader(train_unbalanced_ds, batch_size=len(train_unbalanced_ds), shuffle=True)    # The shuffling is not needed, though\n",
    "train_balanced_dl: DataLoader = DataLoader(train_balanced_ds, batch_size=len(train_balanced_ds), shuffle=True)          # The shuffling is not needed, though"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# The shuffling is not needed, though\n",
    "```\n",
    "\n",
    "Indeed:\n",
    "- We perform *full-dataset batching* (i.e. *non-stochastic* gradient descent);\n",
    "- Gradient accumulation is a summation of gradients.\n",
    "\n",
    "(and sum is commutative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants\n",
    "MODEL_IN_SIZES: List[int] = [6, 2]\n",
    "MODEL_OUT_SIZE: int = 1\n",
    "MODEL_BIAS: bool = True\n",
    "MODEL_DROPOUT: bool = False\n",
    "MODEL_BATCHNORM: bool = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model exactly as described in the paper (with activation function *best-guessed*). To be initialised uniformly, as described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=nn.Sigmoid(),\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")\n",
    "\n",
    "def original_init_(model, extrema: Tuple[float]) -> None:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\"weight\"):\n",
    "            nn.init.uniform_(param, a=extrema[0], b=extrema[1])\n",
    "        if name.endswith(\"bias\"):\n",
    "            nn.init.zeros_(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model just described, to be initialised in a more *modern* fashion (best suited for deep networks, and `PyTorch`'s default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_modern_init = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=nn.Sigmoid(),\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")\n",
    "\n",
    "def modern_init_(model) -> None:\n",
    "    # Already the default in PyTorch\n",
    "    # I.e.: Weights -> Kaiming | Bias -> Uniform with weight-dependent extrema\n",
    "    # See: https://github.com/pytorch/pytorch/blob/7c2103ad5ffdc1ef91231c966988f7f2a61b4166/torch/nn/modules/linear.py#L92\n",
    "    model.reset_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional, incremental improvement upon the model: $\\text{tanh}$ as hidden neurons activation. Popular at the time of publishing (and today, still)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_improved = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=[nn.Tanh(), nn.Sigmoid()],\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model as it was deeper: a $\\text{ReLU}$ network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=[nn.ReLU(), nn.Sigmoid()],\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *bizarre* thing of today: *a* $ReLU$-*like, non-monotonic smooth activation function* in place of the $\\text{ReLU}$.  \n",
    "\n",
    "In our case: the $Mish$ activation function (after [Misra, 2020](https://www.bmvc2020-conference.com/assets/papers/0928.pdf)).  \n",
    "\n",
    "$$ \\text{Mish}(x) = x\\ \\text{tanh}(\\text{ln}(1+\\text{e}^{x})) $$\n",
    "\n",
    "Which has some interesting properties:\n",
    "\n",
    "- Asymptotically a $\\text{ReLU}$, as $x ‚Üí -\\infty$ and $x ‚Üí +\\infty$;\n",
    "- More *expressive* as $x$ approaches zero, preserving *small* negative weights;\n",
    "- Closely related to $\\text{Swish}(x) = \\frac{x}{1 + \\text{e}^{-\\beta x}}$, as $\\frac{d\\ \\text{Mish}(x)}{dx} = \\text{P}(x)\\ \\text{Swish}(x) + \\frac{\\text{Mish}(x)}{x}$ with $\\text{P}(x)$ speculatively acting as a preconditioner for the optimisation problem.\n",
    "\n",
    "![](mish_vs_oth.png)\n",
    "\n",
    "\n",
    "As a fun side-story, the $\\text{Swish}$ function was (re)discovered [in 2017 by Google, via neural architecture search](https://arxiv.org/pdf/1710.05941.pdf), based upon automated benchmarking on the *ImageNet* dataset. It had already been [*discovered* right before](https://arxiv.org/pdf/1702.03118v3.pdf).  \n",
    "\n",
    "Part of the (in)famous series of *Google credit assignment* problems... üò†  \n",
    "(and other [problems, in general](https://www.tensorflow.org/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mish = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=[nn.Mish(), nn.Sigmoid()],\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training until (hopeful) convergence; gather statistics\n",
    "\n",
    "def train_diag_aio(model, dataloader: DataLoader, max_epochs_nr: int, loss, optimizer_fx, optimizer_dict: dict, device) -> Tuple[Tuple, Tuple[float]]:\n",
    "    \n",
    "    optimizer_params: dict = emplace_kv(optimizer_dict, \"params\", model.parameters())\n",
    "    optimizer = argser_f(optimizer_fx, optimizer_params)()\n",
    "\n",
    "\n",
    "    losses: list = []\n",
    "    accuracies: List[float] = []\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    epoch: int\n",
    "    for epoch in range(max_epochs_nr):\n",
    "        \n",
    "        # Iterate over batches\n",
    "        # (in our case: batch == dataset)\n",
    "        x: Tensor\n",
    "        y: Tensor\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            # Move batch to device\n",
    "            x: Tensor\n",
    "            y: Tensor\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            y_hat: Tensor = model(x)\n",
    "            loss_value = loss(y_hat, y)\n",
    "            \n",
    "            # STATISTICS COMPUTATION\n",
    "            # Here is fine, since batch == dataset\n",
    "            with th.no_grad():\n",
    "                pred = th.round(model(x))\n",
    "                accuracy = ((pred.eq(y.view_as(pred))).sum().item() / len(x))\n",
    "\n",
    "            losses.append(loss_value.item())\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return tuple(deepcopy(losses)), tuple(deepcopy(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS_NR: int = int(4e4)\n",
    "LOSS = F.mse_loss\n",
    "OPTIMIZER_FX = optim.SGD\n",
    "OPTIMIZER_PARAMS: dict = {\"params\": None, \"lr\": 0.1, \"momentum\": 0.9}\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "SAMPLE_SIZE: int = 100\n",
    "\n",
    "# Stuff\n",
    "MAX_EPOCHS_NR: int = max(MAX_EPOCHS_NR, 1426)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **the original architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    original_init_(model_original, extrema=(-0.3, 0.3))\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_original, train_unbalanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **the original architecture** on a **balanced dataset**\n",
    "\n",
    "Maybe they forgot to say...  \n",
    "\n",
    "In general: when dealing with an unbalanced dataset, it is always a good idea to balance it. Unless... the *unbalancedness* is an effect of (part of) what you need to predict (e.g. in medical diagnosis!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    original_init_(model_original, extrema=(-0.3, 0.3))\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_original, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a slightly improved architecture**, with ***modern* initialization**, on a **balanced dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_improved)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_improved, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a ReLU Network**, with ***modern* initialization**, on a **balanced dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_relu)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_relu, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a Mish Network**, with ***modern* initialization**, on a **balanced dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_mish)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_mish, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a Mish Network**, with ***modern* initialization**, on the **unbalanced dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_mish)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_mish, train_unbalanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** All code shown today will be available at [`https://github.com/emaballarin/HinTorch`](https://github.com/emaballarin/HinTorch) üë®‚Äçüíª"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a254846949b21e7b07fb88d35b1c9d93d9e8e397acf2db30c111907ad2b4b90b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('RDDL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
