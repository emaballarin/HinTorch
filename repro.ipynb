{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Deep Learning *slipping* [on the shoulders of Giants](https://www.wikiwand.com/en/Standing_on_the_shoulders_of_giants)?\n",
    "### A *critical* reproducibility challenge for [[Rumelhart et al., 1986]](https://sci-hub.se/10.1038/323533a0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TO BE FILLED*\n",
    "\n",
    "Explain why and when it all started..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily compute statistics of arrays\n",
    "import numpy as np\n",
    "\n",
    "# Type hints\n",
    "from typing import Iterable, List, Tuple\n",
    "from torch import Tensor\n",
    "\n",
    "# Tensors and NNs\n",
    "import torch as th\n",
    "from ebtorch.nn import FCBlock  # API for fully-connected NN blocks\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizers\n",
    "import torch.optim as optim\n",
    "\n",
    "# Tensor data[sets|loader]s\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Iterable handling\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "\n",
    "# Utilities for callables\n",
    "from ebtorch.nn.utils import argser_f, emplace_kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithmic function (symmetry-detection for 6-sized binary inputs) we\n",
    "# want to approximate with a NN\n",
    "def is_symmetric(iterable: Iterable) -> float:\n",
    "    assert len(iterable) == 6\n",
    "    if (\n",
    "        # iterable[0:3] == iterable[5:2:-1] still unsupported for PyTorch tensors\n",
    "        iterable[0] == iterable[-1]\n",
    "        and iterable[1] == iterable[-2]\n",
    "        and iterable[2] == iterable[-3]\n",
    "    ):\n",
    "    # 1 == Yes | 0 == No\n",
    "        return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the dataset output-wise early on, to be able to balance it later in\n",
    "# case we need to.\n",
    "\n",
    "x_all: List[Tuple[float]] = [item for item in product([0.0, 1.0], repeat=6)]\n",
    "x_symmetric: List[Tuple[float]] = [item for item in x_all if is_symmetric(item)]\n",
    "x_non_symmetric: List[Tuple[float]] = [item for item in set(x_all).difference(set(x_symmetric))]\n",
    "\n",
    "# And we tensorize it\n",
    "del x_all\n",
    "x_symmetric: Tensor = th.tensor(x_symmetric, dtype=th.float32)\n",
    "x_non_symmetric: Tensor = th.tensor(x_non_symmetric, dtype=th.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unbalanced dataset tensor\n",
    "x: Tensor = th.cat((x_non_symmetric, x_symmetric), dim=0)\n",
    "y: Tensor = th.tensor([[is_symmetric(sub_x)] for sub_x in x])\n",
    "\n",
    "# And the balanced one\n",
    "balancing_ratio: int = int((x_non_symmetric.shape[0]/x_symmetric.shape[0]))\n",
    "x_balanced: Tensor = th.cat((x_non_symmetric, th.cat([x_symmetric]*balancing_ratio, dim=0)), dim=0)\n",
    "y_balanced: Tensor = th.tensor([[is_symmetric(sub_x)] for sub_x in x_balanced])\n",
    "del balancing_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to proper PyTorch data[set|loader]s\n",
    "\n",
    "# Datasets\n",
    "train_unbalanced_ds: TensorDataset = TensorDataset(x, y)\n",
    "train_balanced_ds: TensorDataset = TensorDataset(x_balanced, y_balanced)\n",
    "\n",
    "# Dataloaders (we do full-dataset-batching as in the paper)\n",
    "train_unbalanced_dl: DataLoader = DataLoader(train_unbalanced_ds, batch_size=len(train_unbalanced_ds), shuffle=True)\n",
    "train_balanced_dl: DataLoader = DataLoader(train_balanced_ds, batch_size=len(train_balanced_ds), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants\n",
    "MODEL_IN_SIZES: List[int] = [6, 2]\n",
    "MODEL_OUT_SIZE: int = 1\n",
    "MODEL_BIAS: bool = True\n",
    "MODEL_DROPOUT: bool = False\n",
    "MODEL_BATCHNORM: bool = False\n",
    "\n",
    "# Model definition\n",
    "\n",
    "model_original = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=nn.Sigmoid(),\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")\n",
    "\n",
    "model_modern_init = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=nn.Sigmoid(),\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")\n",
    "\n",
    "model_improved = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=[nn.Tanh(), nn.Sigmoid()],\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")\n",
    "\n",
    "model_relu = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=[nn.ReLU(), nn.Sigmoid()],\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")\n",
    "\n",
    "model_mish = FCBlock(\n",
    "    in_sizes=MODEL_IN_SIZES,\n",
    "    out_size=MODEL_OUT_SIZE,\n",
    "    bias=MODEL_BIAS,\n",
    "    activation_fx=[nn.Mish(), nn.Sigmoid()],\n",
    "    dropout=MODEL_DROPOUT,\n",
    "    batchnorm=MODEL_BATCHNORM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization functions\n",
    "\n",
    "def original_init_(model, extrema: Tuple[float]) -> None:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\"weight\"):\n",
    "            nn.init.uniform_(param, a=extrema[0], b=extrema[1])\n",
    "        if name.endswith(\"bias\"):\n",
    "            nn.init.zeros_(param)\n",
    "\n",
    "def modern_init_(model) -> None:\n",
    "    # Already the default in PyTorch\n",
    "    # I.e.: Weights -> Kaiming | Bias -> Uniform with weight-dependent extrema\n",
    "    # See: https://github.com/pytorch/pytorch/blob/7c2103ad5ffdc1ef91231c966988f7f2a61b4166/torch/nn/modules/linear.py#L92\n",
    "    model.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training until (hopeful) convergence; gather statistics\n",
    "\n",
    "def train_diag_aio(model, dataloader: DataLoader, max_epochs_nr: int, loss, optimizer_fx, optimizer_dict: dict, device) -> Tuple[Tuple, Tuple[float]]:\n",
    "    \n",
    "    optimizer_params: dict = emplace_kv(optimizer_dict, \"params\", model.parameters())\n",
    "    optimizer = argser_f(optimizer_fx, optimizer_params)()\n",
    "\n",
    "\n",
    "    losses: list = []\n",
    "    accuracies: List[float] = []\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    epoch: int\n",
    "    for epoch in range(max_epochs_nr):\n",
    "        \n",
    "        # Iterate over batches\n",
    "        # (in our case: batch == dataset)\n",
    "        x: Tensor\n",
    "        y: Tensor\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            # Move batch to device\n",
    "            x: Tensor\n",
    "            y: Tensor\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            y_hat: Tensor = model(x)\n",
    "            loss_value = loss(y_hat, y)\n",
    "            \n",
    "            # STATISTICS COMPUTATION\n",
    "            # Here is fine, since batch == dataset\n",
    "            with th.no_grad():\n",
    "                pred = th.round(model(x))\n",
    "                accuracy = ((pred.eq(y.view_as(pred))).sum().item() / len(x))\n",
    "\n",
    "            losses.append(loss_value.item())\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return tuple(deepcopy(losses)), tuple(deepcopy(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS_NR: int = int(4e4)\n",
    "LOSS = F.mse_loss\n",
    "OPTIMIZER_FX = optim.SGD\n",
    "OPTIMIZER_PARAMS: dict = {\"params\": None, \"lr\": 0.1, \"momentum\": 0.9}\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "SAMPLE_SIZE: int = 100\n",
    "\n",
    "# Stuff\n",
    "MAX_EPOCHS_NR: int = max(MAX_EPOCHS_NR, 1426)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **the original architecture**\n",
    "\n",
    "Overly simple, a bit naif, but the one that started it all (or not?)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    original_init_(model_original, extrema=(-0.3, 0.3))\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_original, train_unbalanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **the original architecture** on a **balanced dataset**\n",
    "\n",
    "Maybe they forgot to say..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    original_init_(model_original, extrema=(-0.3, 0.3))\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_original, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a slightly improved architecture**, with ***modern* initialization**, on a **balanced dataset**\n",
    "\n",
    "To be honest, it was considered the *standard* NN at the time (with the only exception of the initialization... but that's fine!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_improved)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_improved, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a ReLU Network**, with ***modern* initialization**, on a **balanced dataset**\n",
    "\n",
    "The default of defaults nowadays. It is the same thing Medium is filled up with: will it be up to any good?  \n",
    "(or maybe... despite being a Medium trend 🙃)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_relu)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_relu, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a Mish Network**, with ***modern* initialization**, on a **balanced dataset**\n",
    "\n",
    "Why would someone ever use an activation function that requires exponentiation, is non-monotonic, and looks like a neural action-potential?\n",
    "\n",
    "Hold my beer... 🍻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_mish)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_mish, train_balanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training... **a Mish Network**, with ***modern* initialization**, on the **unbalanced dataset**\n",
    "\n",
    "Who said balancing was necessary?\n",
    "\n",
    "(drops the beer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_epochs_list: List[int] = []\n",
    "accuracies_list: List[float] = []\n",
    "hintonpoint_acc_list: List[float] = []\n",
    "\n",
    "# Iterate over realizations (samples) of training\n",
    "sample_nr: int\n",
    "for sample_nr in range(SAMPLE_SIZE):\n",
    "\n",
    "    # Train model\n",
    "    modern_init_(model_mish)\n",
    "    a: Tuple[float]\n",
    "    _, a = train_diag_aio(model_mish, train_unbalanced_dl, MAX_EPOCHS_NR, LOSS, OPTIMIZER_FX, OPTIMIZER_PARAMS, DEVICE)\n",
    "    \n",
    "    # Compute running stats\n",
    "    accuracies_list.append(a[-1])\n",
    "    hintonpoint_acc_list.append(a[1425])\n",
    "    if a[-1] == 1.0:\n",
    "        converged_epochs_list.append(a.index(1.0))\n",
    "\n",
    "accuracies_np = np.array(accuracies_list)\n",
    "hp_acc_list = np.array(hintonpoint_acc_list)\n",
    "conv_epoch_np = np.array(converged_epochs_list)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"AVERAGE ACCURACY AT {MAX_EPOCHS_NR} EPOCHS: {accuracies_np.mean()} (Std. Dev.: {accuracies_np.std()}) over {len(accuracies_np)} runs\")\n",
    "print(f\"AVERAGE ACCURACY AT HINTON POINT ({int(1425)} EPOCHS): {hp_acc_list.mean()} (Std. Dev.: {hp_acc_list.std()}) over {len(hp_acc_list)} runs\")\n",
    "print(\" \")\n",
    "print(f\"CONVERGED AT {MAX_EPOCHS_NR} EPOCHS: {len(conv_epoch_np)} over {len(accuracies_np)} runs\")\n",
    "print(f\"CONVERGED AT HINTON POINT ({int(1425)} EPOCHS): {(hp_acc_list == 1.0).sum()} over {len(accuracies_np)} runs\")\n",
    "print(\" \")\n",
    "if len(conv_epoch_np) > 0:\n",
    "    print(f\"AVERAGE EPOCHS UNTIL CONVERGENCE: {conv_epoch_np.mean()} (Std. Dev.: {conv_epoch_np.std()})\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a254846949b21e7b07fb88d35b1c9d93d9e8e397acf2db30c111907ad2b4b90b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('RDDL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
